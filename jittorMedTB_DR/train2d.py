import os
# ç¡®ä¿ä½¿ç”¨ GCC 10
os.environ["cc_path"] = "/usr/bin/g++-10"

import jittor as jt
from jittor import nn
from jittor.dataset import Dataset
import numpy as np
import time
import datetime
import json

# ================= 1. 2D è®­ç»ƒé…ç½® =================
jt.flags.use_cuda = 1

DATA_DIR = " " 
SPLIT_JSON = " "
FOLD = 0 

# ğŸ”¥ 2D é…ç½®å˜åŒ–
BATCH_SIZE = 12  # 2D æ¯”è¾ƒå°ï¼Œå¯ä»¥å¼€å¤§ Batch
PATCH_SIZE = (512, 512) # 2D è®­ç»ƒé€šå¸¸æ˜¯å…¨åˆ†è¾¨ç‡åˆ‡ç‰‡æˆ–å¤§ Patch
EPOCHS = 1000
LEARNING_RATE = 1e-3

# ä¿å­˜æ–‡ä»¶ååŒºåˆ†
LOG_FILE = "training_log_2d.txt"
CHECKPOINT_DIR = "./checkpoints_2d"

if not os.path.exists(CHECKPOINT_DIR): os.makedirs(CHECKPOINT_DIR)

# ================= 2. æ—¥å¿—å‡½æ•° =================
def print_log(msg, f_handle=None):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
    content = f"{timestamp}: {msg}"
    print(content)
    if f_handle:
        f_handle.write(content + "\n")
        f_handle.flush()

def print_newline_log(f_handle=None):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
    content = f"{timestamp}: "
    print(content)
    if f_handle:
        f_handle.write(content + "\n")
        f_handle.flush()

# ================= 3. 2D æ•°æ®é›† (æ ¸å¿ƒä¿®æ”¹) =================
class NNUnet2DDataset(Dataset):
    def __init__(self, data_dir, split_json, fold, is_train=True, patch_size=(512, 512), batch_size=12, log_f=None):
        super().__init__()
        self.data_dir = data_dir
        self.patch_size = np.array(patch_size)
        self.batch_size = batch_size
        self.is_train = is_train
        
        with open(split_json, 'r') as f:
            splits = json.load(f)
            
        if fold >= len(splits): raise ValueError(f"Fold {fold} ä¸å­˜åœ¨")
        current_split = splits[fold]
        target_keys = current_split['train'] if is_train else current_split['val']
        
        msg = f"[{'Train' if is_train else 'Val'}] åŠ è½½ Fold {fold}, å…± {len(target_keys)} ä¸ªç—…ä¾‹ (2Dæ¨¡å¼)"
        if log_f: print_log(msg, log_f)

        self.file_list = []
        all_files = os.listdir(data_dir)
        for fname in all_files:
            if fname.endswith('.npy') and '_seg' not in fname:
                case_id = fname.replace('.npy', '')
                if case_id in target_keys:
                    self.file_list.append(fname)
        
        # è®­ç»ƒé›†æ¯ä¸ª epoch è¿­ä»£æ¬¡æ•°
        self.total_len = 250 * batch_size if is_train else len(self.file_list)
        self.set_attrs(batch_size=self.batch_size, total_len=self.total_len, shuffle=is_train)

    def __getitem__(self, index):
        idx = index % len(self.file_list)
        case_id = self.file_list[idx]
        img_path = os.path.join(self.data_dir, case_id)
        seg_path = os.path.join(self.data_dir, case_id.replace('.npy', '_seg.npy'))
        
        # è¯»å– 3D æ•°æ®
        image_3d = np.load(img_path, mmap_mode='r') 
        label_3d = np.load(seg_path, mmap_mode='r') # Shape: [C, D, H, W]
        
        C, D, H, W = image_3d.shape
        
        # ğŸ”¥ æ ¸å¿ƒç­–ç•¥ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªåˆ‡ç‰‡ Z
        selected_z = 0
        
        if self.is_train:
            # è®­ç»ƒæ—¶ï¼š33% çš„æ¦‚ç‡å¼ºåˆ¶é€‰ä¸­æœ‰æ ‡ç­¾çš„å‰æ™¯å±‚ï¼Œé¿å…ä¸€ç›´åœ¨å­¦èƒŒæ™¯
            if np.random.rand() < 0.33:
                # å¯»æ‰¾æœ‰æ ‡ç­¾çš„å±‚
                foreground_slices = np.where(np.sum(label_3d[0], axis=(1,2)) > 0)[0]
                if len(foreground_slices) > 0:
                    selected_z = np.random.choice(foreground_slices)
                else:
                    selected_z = np.random.randint(0, D)
            else:
                selected_z = np.random.randint(0, D)
        else:
            # éªŒè¯æ—¶ï¼šç®€å•å–ä¸­é—´å±‚æˆ–éšæœºå±‚ (éªŒè¯é€šå¸¸åº”è¯¥åœ¨ 3D ä¸Šåšï¼Œè¿™é‡Œç®€åŒ–ä¸ºéšæœºæŠ½ 2D å±‚éªŒè¯ loss)
            selected_z = np.random.randint(0, D)

        # æå– 2D åˆ‡ç‰‡ [C, H, W]
        image_2d = image_3d[:, selected_z, :, :]
        label_2d = label_3d[:, selected_z, :, :]
        
        # 2D éšæœºè£å‰ª
        h_idx = np.random.randint(0, max(1, H - self.patch_size[0]))
        w_idx = np.random.randint(0, max(1, W - self.patch_size[1]))
        
        img_patch = image_2d[:, h_idx:h_idx+self.patch_size[0], w_idx:w_idx+self.patch_size[1]]
        lbl_patch = label_2d[:, h_idx:h_idx+self.patch_size[0], w_idx:w_idx+self.patch_size[1]]
        
        # Padding (å¦‚æœåˆ‡ç‰‡æ¯” patch å°)
        if img_patch.shape[1] < self.patch_size[0] or img_patch.shape[2] < self.patch_size[1]:
            pad = [(0,0)] + [(0, max(0, self.patch_size[i]-img_patch.shape[i+1])) for i in range(2)]
            img_patch = np.pad(img_patch, pad, 'constant')
            lbl_patch = np.pad(lbl_patch, pad, 'constant')

        return np.array(img_patch, dtype=np.float32), (lbl_patch > 0).astype(np.float32)

# ================= 4. æ¨¡å‹ (2D U-Net) =================
class ConvBlock2D(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        # ğŸ”¥ æ”¹ä¸º Conv2d
        self.conv = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1)
        # ğŸ”¥ æ”¹ä¸º InstanceNorm2d
        self.norm = nn.InstanceNorm2d(out_ch, affine=True)
        self.act = nn.LeakyReLU(0.01)
    def execute(self, x): return self.act(self.norm(self.conv(x)))

class UNet2D(nn.Module):
    def __init__(self, in_channels=1, num_classes=1, base=32):
        super().__init__()
        # ç»“æ„é€»è¾‘å’Œ 3D ä¸€æ ·ï¼Œåªæ˜¯ç®—å­æ¢æˆ 2D
        self.enc1 = nn.Sequential(ConvBlock2D(in_channels, base), ConvBlock2D(base, base))
        self.enc2 = nn.Sequential(ConvBlock2D(base, base*2, 2), ConvBlock2D(base*2, base*2))
        self.enc3 = nn.Sequential(ConvBlock2D(base*2, base*4, 2), ConvBlock2D(base*4, base*4))
        self.bottleneck = nn.Sequential(ConvBlock2D(base*4, base*8, 2), ConvBlock2D(base*8, base*8))
        
        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)
        self.dec3 = nn.Sequential(ConvBlock2D(base*8, base*4), ConvBlock2D(base*4, base*4))
        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)
        self.dec2 = nn.Sequential(ConvBlock2D(base*4, base*2), ConvBlock2D(base*2, base*2))
        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)
        self.dec1 = nn.Sequential(ConvBlock2D(base*2, base), ConvBlock2D(base, base))
        self.final = nn.Conv2d(base, num_classes, 1)

    def execute(self, x):
        x1 = self.enc1(x)
        x2 = self.enc2(x1)
        x3 = self.enc3(x2)
        x_bot = self.bottleneck(x3)
        u3 = self.up3(x_bot)
        if u3.shape!=x3.shape: u3=u3[:,:,:x3.shape[2],:x3.shape[3]] # 2D åªæœ‰ H, W
        d3 = self.dec3(jt.contrib.concat([u3, x3], dim=1))
        u2 = self.up2(d3)
        if u2.shape!=x2.shape: u2=u2[:,:,:x2.shape[2],:x2.shape[3]]
        d2 = self.dec2(jt.contrib.concat([u2, x2], dim=1))
        u1 = self.up1(d2)
        if u1.shape!=x1.shape: u1=u1[:,:,:x1.shape[2],:x1.shape[3]]
        d1 = self.dec1(jt.contrib.concat([u1, x1], dim=1))
        return self.final(d1)

# ================= 5. Loss (2Dç‰ˆ) =================
def soft_dice_loss(outputs, targets):
    probs = jt.sigmoid(outputs)
    # ğŸ”¥ 2D åªåœ¨ [2, 3] ç»´åº¦æ±‚å’Œ (Batch, Channel, H, W)
    inter = (probs * targets).sum(dims=[2,3])
    union = probs.sum(dims=[2,3]) + targets.sum(dims=[2,3])
    dice = (2. * inter + 1e-5) / (union + 1e-5)
    return -dice.mean()

def calculate_dice(outputs, targets):
    probs = (jt.sigmoid(outputs) > 0.5).float()
    inter = (probs * targets).sum()
    union = probs.sum() + targets.sum()
    return (2. * inter + 1e-5) / (union + 1e-5)

# ================= 6. ä¸»ç¨‹åº =================
if __name__ == "__main__":
    log_file = open(LOG_FILE, "a")
    
    print_log(f"Loading 2D Training Task...", log_file)
    train_ds = NNUnet2DDataset(DATA_DIR, SPLIT_JSON, FOLD, is_train=True, patch_size=PATCH_SIZE, batch_size=BATCH_SIZE, log_f=log_file)
    val_ds = NNUnet2DDataset(DATA_DIR, SPLIT_JSON, FOLD, is_train=False, patch_size=PATCH_SIZE, batch_size=BATCH_SIZE, log_f=log_file)
    
    model = UNet2D()
    optimizer = nn.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.99, nesterov=True)
    
    # æ–­ç‚¹ç»­è®­é€»è¾‘ (é’ˆå¯¹ 2D checkpoint)
    start_epoch = 0
    best_dice = 0.0
    latest_ckpt_path = os.path.join(CHECKPOINT_DIR, "checkpoint_2d_latest.pkl")
    best_ckpt_path = os.path.join(CHECKPOINT_DIR, "checkpoint_2d_best.pkl")
    
    if os.path.exists(latest_ckpt_path):
        print_log(f"âš ï¸ Found checkpoint: {latest_ckpt_path}, Resuming...", log_file)
        checkpoint = jt.load(latest_ckpt_path)
        model.load_state_dict(checkpoint['model_state'])
        optimizer.load_state_dict(checkpoint['optimizer_state'])
        start_epoch = checkpoint['epoch'] + 1
        best_dice = checkpoint['best_dice']
        print_log(f"âœ… Resumed from Epoch {start_epoch-1}. Best Dice: {best_dice:.4f}", log_file)
    else:
        print_log("ğŸš€ Starting fresh 2D training.", log_file)

    for epoch in range(start_epoch, EPOCHS):
        print_newline_log(log_file)
        print_log(f"Epoch {epoch}", log_file)
        
        optimizer.lr = LEARNING_RATE * (1 - epoch/EPOCHS)**0.9
        print_log(f"Current learning rate: {optimizer.lr:.4f}", log_file)
        
        ep_start = time.time()
        train_loss_list = []
        model.train()
        for imgs, masks in train_ds:
            pred = model(imgs)
            loss = soft_dice_loss(pred, masks)
            optimizer.step(loss)
            train_loss_list.append(loss.item())
        
        val_loss_list = []
        dice_list = []
        model.eval()
        with jt.no_grad():
            for imgs, masks in val_ds:
                pred = model(imgs)
                v_loss = soft_dice_loss(pred, masks)
                dice = calculate_dice(pred, masks)
                val_loss_list.append(v_loss.item())
                dice_list.append(dice.item())
        
        avg_train_loss = np.mean(train_loss_list)
        avg_val_loss = np.mean(val_loss_list)
        avg_dice = np.mean(dice_list)
        dur = time.time() - ep_start
        
        print_log(f"train_loss {avg_train_loss:.4f}", log_file)
        print_log(f"val_loss {avg_val_loss:.4f}", log_file)
        print_log(f"Pseudo dice [{avg_dice:.4f}]", log_file)
        print_log(f"Epoch time: {dur:.2f} s", log_file)
        
        checkpoint_data = {
            'epoch': epoch,
            'model_state': model.state_dict(),
            'optimizer_state': optimizer.state_dict(),
            'best_dice': best_dice
        }
        
        jt.save(checkpoint_data, latest_ckpt_path)
        
        if avg_dice > best_dice:
            best_dice = avg_dice
            print_log(f"â­ New Best Dice: {best_dice:.4f}", log_file)
            jt.save(checkpoint_data, best_ckpt_path)
        
        if (epoch + 1) % 50 == 0:
            archive_path = os.path.join(CHECKPOINT_DIR, f"checkpoint_2d_ep{epoch}.pkl")
            jt.save(checkpoint_data, archive_path)
            
    log_file.close()